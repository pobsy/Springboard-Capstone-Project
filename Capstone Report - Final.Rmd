---
title: "Capstone Milestone Report"
author: "Paddy"
date: "20 May 2017"
output: 
        html_document:
          keep_md: true

---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "README_figs/README-"
)
```

# Introduction  

## What is Lending Club
The client is Lending Club, an online credit marketplace that uses technology to offer more attractive loan terms than traditional banks, passing the savings on to borrowers in the form of lower interest rates and invetors in terms of solid returns.

Lending Club leverages online data and technology to quickly assess the risk of the application and if the customer is successful, determine the applicant's credit score and assign an appropriate interest rate. Like with any bank, if the applicant's risk is deemed to be too high, the loan is rejected. 

Lending Club offers a quick assessement and easier application than traditional banks and passes savings on to borrowers in terms of lower interest rates. For investors,  it provides risk adjusted returns by allowing for investment diversification across a range of loans varying in risk. 

Lending Club has grown from just over 100,000,000 dollars across 10,449 loans in 2010 to over 26.6 billion dollars worth of loans across over 2 million loans in quarter 1 of 2017. 

## The Problem 
One of Lending Club's biggest challenges is ensuring that repayments are made and trying to filter out delinquent/default loans through robust screening. As of June 2015, Lending Club's annual default rate across all grades was about 7%. A reduction in the default rate will reduce the risk of the loans being funded by investors making the investment more attractive. 

The goal of this project is to determine the key indicators and attributes of successful loans and hence create a model that offers a more robust screening process of applicants and hence reduce the overall percentage of default/delinquant loans. This will allow Lending Club to more accurately determine likely default loan applicants and hence achieve a lower default rate percentage on their loans. This reduced risk can be more attractive to investors and hence can increase the overall investment in Lending Club.


# The Datasets

2 datasets will be used for the purpose of this project:

  1. Approved Data - this contains 42,000 plus observations for loans that were issued between 2007 and 2011. The original data set on the Lending Club website did not include the FICO scores of the borrowers - a vital feature particularly in the comparative analysis with other data sets. A previous data set containing this information was obtained during the literature review. The following are the main features that will be used in the data set:
  
      * Amount requested
      * Term of loan - 36 or 60 months
      * Interest assigned to the loan - this is calculated by Lending Club after assessing the risk and determining a credit           rating.
      * Grade assigned to the loan - there are 7 different grades (A-G) varying depending on the loan risk
      * FICO score - this is a type of credit score and is used by lenders as an indicator of the risk of an application
      * Previous delinquency history
      * Debt-to-Income (DTI) ratio - total monthly debt repayment (excluding mortage repayments) divided by the monthly income.         A lower value is more desirable. 
      * Loan status - 7 indicators which show if if the loan has been complete, if its late, on track or default.This is a             vital feature to have for this analysis and will be very useful in the development of the predictive model.
      
  2. Rejected Data - this contains a massive 755,000 plus observations for loan applications that were rejected by Lending Club between 2007 and 2012. Although there is a significant number of observations there is less features available than with the Approved Data. The followinga are the Rejected Data features (which are all also used in the Approved Data):
  
      * Amount requested
      * Application date
      * Loan title
      * Risk score - FICO is used as the measurement for loan requests up until Nov-13. Since the data chosen is up to 2013,           FICO is the measure used and hence is comparable with the Approved Data set. 
      * DTI ratio
      * Zip code
      * State
      * Employment length
      * Policy code
      
## Data Limitations

The main limitations with the 2 data sets is the lack of common features. The Rejected Data set only has 9 features which can be compared to the Approved Data set. Features such as Previous Delinquency History, Days Delinquent and other variables that give a better insight into the applicants credit history would be useful for the Rejected Data set. Although Loan Title is included in both data sets, the lack of the Purpose feature will also be a limitation as this column has far fewer variables and hence can be easily grouped into a manageable number of observations for the comparative analysis. 

From the initial exploration of the available data, the following features will need to be analysed in greater detail and will be useful for the comparative analysis:

      * FICO
      * DTI
      * Amount
      * Employment length


## Data Wrangling and Cleaning 

The data sets were relatively comprehensive and well put together and hence did not require any major transformations. 

However, there were quite a few features that would not be required for the analysis, along with features that were made up primarily of NA values. Formatting was required on several columns as well as the application of One Hot Encoding which would aid the predictive model testing later in the project. 

The main libraries required for the data wrangling and cleaning were acquired when the relevant data set was imported. 

```{r eval =FALSE}
library(tidyr)
library(dplyr)
library(readr)
LC_data <- read_csv("~/Springboard/Capstone project/LC data.csv")

```
The following shows the steps undertaken to transform the data into a desirable format suitable for the analysis.

### Deleting features
The first task was to delete any obvious columns that would not be of any benefit for the analysis. Once this was done, further investigation into each feature was carried out to determine the percentage of "NA" fields, deleting anything greater than 80% NA.  

```{r eval = FALSE}
lc = LC_data
missing_values <- sapply(lc, function(x) {
  percentage <- sum(is.na(x))/length(x)
  percentage < 0.2
  })
lc <- lc[, missing_values == TRUE]
lc
```
Any features with only one unique value were also deleted from the data set, as these would not add any value to the analysis.

```{r eval = FALSE}
unique_values2 <- sapply(lc, function(x) {
size <- length(unique(x[!is.na(x)]))
size > 1
   })
lc <- lc[, unique_values2 == TRUE]
```
### Formatting
The main formatting carried out on the data sets involved transforming variables from a character to a numberic format. This first involved removing any non numeric text/values such as "yrs", "months" and "%", "+" and ">" signs. The main variables that were formatted in this manner were interest rate, term, employment length and revolving utilisation.

```{r eval = FALSE}
lc$term <- gsub("months", "", lc$term)
lc$term <- as.numeric(lc$term)
```
The issue date feature "issue_d" was also formatted from a "character" to a "date" to make it compatible with the date column from the rejected data. 

One of the most valuable features from this data set is "loan status". This categorises each loan into one of seven observations to represent the current status of the loan. For the purpose of the analysis, these status' were separated into 2 groups - good indicators that represent if a loan is still on track to be fully paid off without default or delay, and bad indicators which represent default or delay. These 2 indicator groups were then assigned a binary in the new column "bad_indicators". This will be very helpful in the analysis, particularly for the development of the predictive model.

```{r eval = FALSE}
bad_indicators <- c("Late (16-30 days)", "Late (31-120 days)", "Default", "Charged Off")
lc$is_bad <- ifelse(lc$loan_status %in% bad_indicators, 1,
                    ifelse(lc$loan_status=="", NA,
                   0))
```
### Data normalisation and One Hot Encoding
In order to improve prediction accuracy in the predictive model, the FICO feature was normalised. The standardizing method was used as it allows for centre and scale to be specified in the code and hence, is easily transferable between different data sets (i.e. between approved and rejected data). 

```{r eval = FALSE}
lc$fico_norm <- scale(lc$fico_range_mean, center = TRUE, scale = TRUE)

```
The final step in the data wrangling was to apply One Hot Encoding to features of paricular interest. The rule of thumb is that OHE can be applied to features of between 2 and 30 unique values. 

```{r eval =FALSE}

library(ade4)

lc_OHE = lc
  ohe_feats = c('term_mths', 'grade', 'emp_length_yrs', 'home_ownership', 'verification_status',   'loan_status', 'pymnt_plan', 'purpose', 'delinq_2yrs', 'inq_last_6mths', 'pub_rec', 'pub_rec_bankruptcies')
for(f in ohe_feats) {
     df_dummy1 = acm.disjonctif(as.data.frame(as.factor(unlist(lc[f])))) 
     lc_OHE = cbind(lc_OHE, df_dummy1)

}
```

The addition of OHE is to facilitate the analysis and testing in the predictive modelling section of the project. For the purpose of the initial exploratory analysis and data visualisation, the original columns will be used. 


Similiar wrangling was carried out for the rejected loan data set. 

### Approved and Rejected Data - Combined Data Set
The 2 data sets were then combined using the union() function. Firstly, a "status" feature was added to both data sets, which was set to either "rejected" or "approved" depending on the data set. Just the relevant columns were selected from the approved data set, and the names of both columns were changed to allow for the columns to be joined together. 

```{r eval = FALSE}
lc_comparison = lc
lc_comparison <- select(lc_comparison, emp_length_yrs, loan_amnt, issue_d, fico_range_mean, loan_status, dti)
lc_comparison <- lc_comparison  %>% mutate("Status" = "approved")
lc_comp <- union(lcr, lc_comparison)
```
To complete the combined data set wrangling, the FICO score was normalised as before with the approved data set. There was no need to specify the centre and scale as this encompasses all observations for both accepted and rejected loans. 


## Data Exploration
The purpose of the inital exploration was to gauge the relationship between the loan status and the remaining variables in the approved data set to determine if there are any factors that have more of an influence on the status of a loan than others. The objective of this was to take a selection of the more influencive variables in the data set to crudely determine the variables that could have the biggest impact on the success on the loan. 

Once determined, the focus of the further data visualisation and exploration will be to deep dive into these variables and determine how they interact with one another. 

The following are the variables to be investigated in this section:
      * fico_mean                       * grade                       
      * dti                             * home ownership
      * loan amnt                       * ann_income
      * emp length                      * inq. last 6 mths
      * term                            * revol_util
      * int rate                        * purpose

This will be done by plotting the density of 2 factors in is_bad (0/good/red and 1/bad/blue)) against each of the variables of interest. 
      
### Fico_mean
  
```{r eval = TRUE, warning= FALSE, error = FALSE, message = FALSE}
#install.packages("readr")
#library(readr)
lc_cleaned <- read.csv("~/Springboard/Capstone project/New folder/lc_cleaned.csv")

lc = lc_cleaned


library(ggplot2)

p <- ggplot(lc, aes(x = fico_range_mean, group = is_bad, color = factor(is_bad))) + geom_density()
p
```

As expected there is a difference between the 2 status' for this variable which suggests that a higher FICO score is more desirable with higher rates of defaulted loans witnessed in lower FICO ranges.

### DTI
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}

p1 <- ggplot(lc, aes(x = dti, group = is_bad, color = factor(is_bad))) + geom_density()
p1  
```
DTI is quite evenly spread however there is a slight tendency of higher DTI's to result in more bad loans hence confirming that the lower the DTI value the more likely the loan is to be successfully paid off. It is worth looking into further. 

### Loan Amount
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p2 <- ggplot(lc, aes(x = loan_amnt, group = is_bad, color = factor(is_bad))) + geom_density()
p2
```
Apart from loans of smaller amounts having more of a success of being fully paid off, the spread is quite even along the graph and does not suggest much of a relationship between the chance of failure and success.

### Employment length
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p3 <- ggplot(lc, aes(x = emp_length_yrs, group = is_bad, color = factor(is_bad))) + geom_density()
p3
```
Similar to Loan Amount, apart from 10 plus years of employment, there is no stark difference between the 2 observations. 

### Term
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p4 <- ggplot(lc, aes(x = term_mths, group = is_bad, color = factor(is_bad))) + geom_density()
p4
```
Interestingly, the shorter the term, the more likely the loan is to succeed. This is definitely worth looking into further. 

### Interest Rate
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p5 <- ggplot(lc, aes(x = int_rate_percent, group = is_bad, color = factor(is_bad))) + geom_density()
p5
```
Similar to DTI, it would be expected that the lower the interest rate on the loan, the greater the chance of success as the interest rate is assigned based on the risk of the loan which is derived from the applicant's credit history. 

### Grade
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p6 <- ggplot(lc, aes(x = grade, group = is_bad, color = factor(is_bad))) + geom_density()
p6
```
Grade appears to be similar to the interest rate in the fact that the grades which are deemed to be less risky are less likely to default i.e. A-grades are less likely to default while G-grades are more likely to default. It may be worth looking at the relationship between grade and interest rate later in the data exploration to determine the correlation between the two. 

### Home Ownership
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p7 <- ggplot(lc, aes(x = home_ownership, group = is_bad, color = factor(is_bad))) + geom_density()
p7
```
The two main observations in "home_ownership" are "rent" and "mortgage". The expectation would be that home owners would be more likely to repay their loans (which is the case) while renters would be more likely to default (which is not the case). There is not as much of a difference between the 2 of these observations as expected.  

### Annual Income
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p8 <- ggplot(lc, aes(x = annual_inc, group = is_bad, color = factor(is_bad))) + geom_density()
p8
```
There appears to be a consistent correlation between incomes for good and bad loans hence, this will not be investigated further.  

### Inquiries in the last 6 months
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p9 <- ggplot(lc, aes(x = inq_last_6mths, group = is_bad, color = factor(is_bad))) + geom_density()
p9
```
This graph shows that if the applicant has 0 inquiries they have a better chance of a good loan. There is a relatively consistent trend after this where there is not a major difference between the number of inquiries and the success of the loan.

### Revolution util
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p10 <- ggplot(lc, aes(x = revol_util, group = is_bad, color = factor(is_bad))) + geom_density()
p10
```
There is a very clear difference here between good and bad loans. The graph suggests that the lower the Revol Util, the better chance of successfully repaying the loan. 

### Purpose
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p11 <- ggplot(lc, aes(x = purpose, group = is_bad, color = factor(is_bad))) + geom_density()
p11
```
There are 4 main peaks in the graph with significant differences between the good and bad loan status. This will be looked at in more detail to determine what purpose of the loan results in a greater risk. 


_________________________________


## Import the data set
I will be using the "lc_formatted" dataset as described in the Data Wrangling section. This dataset does not include the One Hot Encoding exercise that was carried out on the data. This may have been useful if a significant amount of variables were being used for the machine learning however for the purpose of the logistic regression, I will be selecting the variables that will be used. Logistic regression will format the variables as required.

Import the data set and rename to lc1 (for ease of use):

```{r eval =TRUE, warning = FALSE}
library(tidyr)
library(dplyr)
library(readr)

lc_formatted <- read_csv("~/Springboard/Capstone project/New folder/lc_formatted.csv")

lc2 = lc_formatted
```

## Further formatting

### Variables not required

For the date columns to be useful, they need to be binned into quarters or even months so that trends can be investigated in groups rather than on particular days. For this analysis, the date columns will not be investigated, however future work could investigate correlations between default rate and loans issued on certain dates for example. Hence, columns "issue date", "last_pymnt_d", "last_credit_pull_d" etc will be omitted from the regression in this instance. 

Address will also not be required for the purpose of this analysis so this will also be removed from the data set. 

```{r eval = TRUE}
lc2[, c("addr_state", "earliest_cr_line", "last_credit_pull_d", "last_pymnt_d", "X1")] <- NULL
```
## Logistic Regression

The initial logistic regression to be carried out is to gain a better insight into the default rates of loans. The probability of default will be calculated for each loan which will allow for further analysis in terms of determining the true return on investment of a loan (when the default is taken into account as well as the interest rate - this will allow for a more robust portfolio for investors and hence a better overall return on investment). Additionally, further analysis will be carried out to determine the factors that were used by Lending Club to predict interest rates. 


The dependent variable which will be used for the inital logistic regression analysis is "is_bad", the variable which tells us whether or not the loan has defaulted. "1" means the loan has defaulted while "0" means the loan has been successfully paid off. The independent variables will be added around this dependent variable and re-iterated until a successful model has been created.

Once the model has been built, the "Test" dataset will be used for its evalution. 


## Step 1 - Get the Baseline 
Predict the most frequent outcome (i.e. default loan or complete loan) of all observations. This is done by counting the actual number of default loans vs the number of complete loans to give the accuracy of the dataset. 

```{r eval = TRUE, echo = FALSE, results = 'asis'}
cm_baseline = table(lc2$is_bad, sign(lc2$is_bad))
knitr :: kable(cm_baseline)
```
This produces the following table. The use of the sign function can derive a smarter baseline however in this case, both tables (actual results and smart baseline) derive the same accuracy of 84.88%.

Looking at the table and results however, the baseline of 84.88% is quite high due to the fact that the majority of cases are 0. To make the prediction more insightful, the data will be split to a smaller data set which will comprise 50% 0 values and 50% 1 values. This will give the prediction alot more value.
  

## Step 2 - Split the data 
Using CaTools package, split the data into 2 sections - test data and train data. The ratio for the divide will be 80/20 i.e. 80% of the dataset will be made up of train data "Train" and the remaining 20% will be test data "Test".

```{r eval = TRUE}
library(caTools)
library(ROSE)
set.seed(80)
split = sample.split(lc2$is_bad, SplitRatio = 0.80)
Train = subset(lc2, split = TRUE)
Train_under <- ovun.sample(is_bad~., data = lc2, method="under", N = 12000, seed = 40)$data
Test = subset(lc2, split = FALSE)
```

Train data is the data set based on the original lc2 data. Alongside this data set, Train_under will also be used as this will give a clearer indication of the accuracy of the model that is built for reasons discussed in step 1.

## Step 3 - Run logistic regression on the model

Model1 will take into account all variables apart from member id and loan status. The idea is to start with the majority of the variables and gradually narrow down until only the significant variables are included in the model.
```{r }
 model1 <- glm(is_bad ~. -id -loan_status, data = Train, family = "binomial")
summary1 = summary(model1)
```

Model1 gave several significant values. The next step is to take just the significant values and run the regression model again until all values in the model are significant. I've left fico_norm in as this is a significant variable.

```{r}
 model2 <- glm(is_bad ~ loan_amnt + term_mths + int_rate_percent + installment + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train, family = "binomial")
summary2 = summary(model2)

```


```{r}
model3 <- glm(is_bad ~ loan_amnt + term_mths + int_rate_percent + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train, family = "binomial")

summary3 = summary(model3)
```


```{r}
model4 <- glm(is_bad ~ loan_amnt + term_mths + purpose + int_rate_percent + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train, family = "binomial")

summary4 = summary(model4)
```
Running this model on the Train1 data to see how it compares:

```{r}
model5 <- glm(is_bad ~ loan_amnt + term_mths + purpose + int_rate_percent + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train_under, family = "binomial")

summary5 = summary(model5)
```
After several iterations, model4 is a good fit and delivers an accurate model and hence will be used for predicting. Comparing this to model 5 (where the undersampled data is used), the accuracy has improved based on the AIC score reduction from 24716 in the Train data to 9927.4 in the Train_under data. Similarly, the goodness of fit of the model has improved as both the NULL deviance and residual deviance have reduced with the use of the Train_under data. 

## Step 4 - Predicting on Training Data

As model5 was strong, this will be chosen for the preiction stage on the Train data. The na's will be removed to ensure all columns are of equal length. The resulting model6 will be used to obtain a confusion matrix for analysis.

```{r}
model6 <- glm(is_bad ~ loan_amnt + term_mths + purpose + int_rate_percent + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train_under, family = "binomial", na.action = na.exclude)
summary(model6)

```
Use the predict function on the Train data with a threshold value of 0.5 to begin with. This will be adjusted for optimal position.
```{r eval = TRUE, echo = FALSE, results = 'asis'}

predict1 = predict(model6, type = "response")
cmTrain <- table(Train$is_bad, predict1 > 0.5)
knitr :: kable(cm_baseline)

#possibly leave this out - only include test data when evaluating
#predictTest1 = predict(model7, type = "response", newdata = Test)
#cmTest <- table(Test$is_bad, predictTest1 > 0.5)
```
This matrix allows for calculations to be made regarding the accuracy of the model.
          
## Step 5 - Measuring the accuracy of the model
From the table above, the accuracy of the model can be measured using the following metrics.

### Overall accuracy 
Overall accuracy = (TN + TP)/N

```{r}
(34620+2438)/nrow(Train)

```
This gives an accuracy of 0.8711535 or 87.12%.

### Sensitivity 
Sensitivity = TP/(TP+FN)

```{r}
2438/(2438+3991)
```
Sensitivity = 0.37922 or 37.92%

### Specificity
Specificity = TN/(TN+FP)

```{r}
34620/(34620+1457)

```
****Need to update the results****Specificity = 0.9596142 or 95.96%

Comparing the accuracy of the model to the baseline model shows the model is 2.24% more accurate i.e. 
Baseline accuracy = 84.88% 
Model = 87.12%

Hence, the calculations show that the model is more accurate than the baseline model which means that the model will derive less mistakes when predicting the outcome of the success of a loan.

Want to go a step further with the analysis and investigate the threshold value (t) and the impact it can have on the accuracy and effectiveness of the model.

## Step 6 Analysis

### AUC
In order to investigate the t value further, a good place to start is to determine the area under the curve (AUC) of the ROC curve for the model. The resulting AUC value is an indicator of the discrimination ability of the model. The value ranges from 0.5 to 1 with a higher value representing a better discrimination ability and hence a better predictor. 


For example, an ROC curve with an AUC of 0.5 will have a curve close to 45 degrees and will have little or no discimination ability i.e. the prediction will be 50/50 - pure chance.
Whereas a curve that tends towards the top left hand corner of the plot will have an AUC of closer to 1 which indicates perfect discrimination ability.
Hence, a high AUC value is more desirable as it indicate a better predictor power for the model in question.

```{r}
#![AUC Explanation.](\C:\Users\Pat\Documents\Springboard\Capstone project\Capstone #Milestone\README_figs_lreg\AUC.png)
```

Using the Train data to get the AUC:

```{r}
library(ROCR)
pred = ROCR::prediction(predict1, Train$is_bad)
as.numeric(performance(pred, "auc")@y.values)
```
This gives a value of 0.8917 or 89.17% - this is higher than our baseline (84.88%) but can still be improved.

For a bank, the cost of the misclassifiction of default loans is much higher than the cost of the misclassification of successful loans i.e. it is more detrimental to Lending Club's business and to investors if default loans are predicted as successful rather than successful loans being predicted as default. 

Hence, for this particular model, sensitivity is very important to ensure the number of False negative cases in the model is kept as low as possible. This can be improved by adjusting the t value:

Investigating overall accuracy, sensivity, and specificity for different t values:

```{r}
predict1 = predict(model7, type = "response")
table(Train$is_bad, predict1 > 0.3)
table(Train$is_bad, predict1 > 0.4)
table(Train$is_bad, predict1 > 0.5)
table(Train$is_bad, predict1 > 0.75)
```
Computing these values and compiling the data into a table:

```{r}
tvalue <- matrix(c(0.598, 0.483, 0.3797, 0.132, 0.911, 0.941, 0.9596, 0.992, 0.863, 0.870, 0.871, 0.861), ncol = 4, byrow= FALSE)
colnames(tvalue) <- c("t=0.3", "t=0.4", "t=0.5", "t=0.75")
rownames(tvalue) <- c("sensitivity", "specificity", "accuracy")
tvalue <- as.table(tvalue)
tvalue
```
    FALSE  TRUE
  0 32877  3200
  1  2585  3844
   
    FALSE  TRUE
  0 33928  2149
  1  3327  3102
   
    FALSE  TRUE
  0 34620  1457
  1  3988  2441
   
    FALSE  TRUE
  0 35772   305
  1  5581   848
   
   
             t=0.3  t=0.4  t=0.5 t=0.75
sensitivity 0.5980 0.1320 0.9596 0.8700
specificity 0.4830 0.9110 0.9920 0.8710
accuracy    0.3797 0.9410 0.8630 0.8610


The main priority in this case is keeping the number of FN cases as low as possible. FN means that the model predicted the loan was good while in reality, the loan was actaully bad. There will always be some percentage of loans where the loan is predicted good but is actually bad however, we can adjust the t value to ensure this value is kept as low as possible.

Using t=0.3 gives a relatively low FN rate in comparison to other higher t-values however, the overall percentage accuracy is still relatively high (1% lower than t=0.5 and t=0.75) - hence, t=0.3 seems to be the most suitable value to use. This can be confirmed by visually inspecting the ROC curve.


### ROC Curve
In the calculations above, the t value of 0.5 was used. In this section, the ROC curve will be graphed which will allow for the t value to be investigated further. When choosing the t value, there is a trade off between sensitivity and specificity calculations. Hence, what is more detrimental to the success of Lending Club from the model - the cost of failing to detect positives or the cost of raising false alarms. 

Graphing the curve:

```{r}
ROCRpred = prediction(predict1, Train$is_bad)
ROCRperf = performance(ROCRpred, "tpr", "fpr")
p1 <- plot(ROCRperf)

png("ROC_curve.png")
p2 <- plot(ROCRperf, colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj = c(-0.2, 1.7))
p2
dev.off()
```
*****remember to include the graph*****


After investigating different values for the threshold and visually inspecting the ROC curve, overall the most suitable t-value is 0.30 so this will be used to determine how well the model works using test data.



## Step 7 - Testing the Model 

Computing the out-of-sample metrics on the Test dataset:

```{r}
predictTest = predict(model7, type = "response", newdata = Test)
table(Test$is_bad, predictTest > 0.3)
```

       
    FALSE  TRUE
  0 32875  3202
  1  2581  3848
  
Overall accuracy = (TN + TP)/N
    = (32875+3848)/nrow(Test)
    = 0.8632784

Sensitivity = TP/(TP+FN)
    = 3848/(3848+2581)
    = 0.5985379

Specificity = TN/(TN+FP)
    = 32875/(32875+3202)
    = 0.91124

At a cutoff of 0.3, the sensitivity is massively improved from 0.38 in the original model to 0.59 in the revised model. This does not come with a massive trade off in overall model accuracy. The overall accuracy of the revised model on the Test set is 0.863 which is only slightly off the accuracy of the original model of 0.8711. Despite this slight descrease, the revised model is still more accurate than the baseline which was 0.8488. 

Finish this section of linear regression by adding the default rates to each observation in the lc1 data set:

```{r}
lc1$predicted.risk <- predict(model7, type = "response")
```


## Step 8 - Using the Model

### Beating Lending Club's model
For an investor, the rate of return on an investment is derived from the interest rate. This is derived from a series of factors such as the FICO score, employment length, etc. 


Hence, the interest rate is essentially an indication of the risk associated with a loan.
***gradient boosting to be carried out using interest_rate as the dependent variable to determine the contribution of each variable in calculating the interest rate - this gbm model can potentially be compared to glm model for int rate to prove the gbm model is more accurate***

Overall, the interest rate is not an accurate indicator of the return on investment as it only takes into account the return if the loan is successful. Hence, to determine the expected return on the loan or "Expected rate of return", the risk on each loan must also be included. 

The default rate or probability of default of each loan was calculated for each loan in the training set and is derived  from the model7:

```{r}
Train$predicted.risk <- predict(model7, type = "response")
```

### Determine the expected rate of return on the investment


```{r}
ERR <- function(x) {
            ((1-lc1$predicted.risk)*((lc1$int_rate_percent)/100) + (lc1$predicted.risk)*(-1))
}
```

lc1$ERR <- sapply(lc1, ERR)

These loans can then be investigated further using the ERR metric and default rate in order to set the most suitable threshold for accepting loans - the lower the threshold, the easier it is to get a loan and hence, the more default loans that would be expected.


### Important variables
Using the log of the coefficients, can see which are the most important factors to consider when investigating the success of a loan. For investors, these factors can then be weighted to determine the highest rate of return on an investment portfolio.

