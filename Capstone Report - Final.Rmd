---
title: "Capstone Milestone Report"
author: "Paddy"
date: "20 May 2017"
output:
        html_document:
          keep_md: true
  
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "README_figs_final/README-"
)
```

# Introduction  

## What is Lending Club
The client is Lending Club, an online credit marketplace that uses technology to offer more attractive loan terms than traditional banks, passing the savings on to borrowers in the form of lower interest rates and invetors in terms of solid returns.

Lending Club leverages online data and technology to quickly assess the risk of the application and if the customer is successful, determine the applicant's credit score and assign an appropriate interest rate. Like with any bank, if the applicant's risk is deemed to be too high, the loan is rejected. 

Lending Club offers a quick assessement and easier application than traditional banks and passes savings on to borrowers in terms of lower interest rates. For investors,  it provides risk adjusted returns by allowing for investment diversification across a range of loans varying in risk. 

Lending Club has grown from just over 100,000,000 dollars across 10,449 loans in 2010 to over 26.6 billion dollars worth of loans across over 2 million loans in quarter 1 of 2017. 

## The Problem 
One of Lending Club's biggest challenges is ensuring that repayments are made and trying to filter out delinquent/default loans through robust screening. As of June 2015, Lending Club's annual default rate across all grades was about 7%. A reduction in the default rate will reduce the risk of the loans being funded by investors making the investment more attractive. 

The goal of this project is to determine the key indicators and attributes of successful loans and hence create a model that offers a more robust screening process of applicants and hence reduce the overall percentage of default/delinquant loans. This will allow Lending Club to more accurately determine likely default loan applicants and hence achieve a lower default rate percentage on their loans. This reduced risk can be more attractive to investors and hence can increase the overall investment in Lending Club.


# The Datasets

2 datasets will be used for the purpose of this project:

  1. Approved Data - this contains 42,000 plus observations for loans that were issued between 2007 and 2011. The original data set on the Lending Club website did not include the FICO scores of the borrowers - a vital feature particularly in the comparative analysis with other data sets. A previous data set containing this information was obtained during the literature review. The following are the main features that will be used in the data set:
  
      * Amount requested
      * Term of loan - 36 or 60 months
      * Interest assigned to the loan - this is calculated by Lending Club after assessing the risk and determining a credit           rating.
      * Grade assigned to the loan - there are 7 different grades (A-G) varying depending on the loan risk
      * FICO score - this is a type of credit score and is used by lenders as an indicator of the risk of an application
      * Previous delinquency history
      * Debt-to-Income (DTI) ratio - total monthly debt repayment (excluding mortage repayments) divided by the monthly income.         A lower value is more desirable. 
      * Loan status - 7 indicators which show if if the loan has been complete, if its late, on track or default.This is a             vital feature to have for this analysis and will be very useful in the development of the predictive model.
      
  2. Rejected Data - this contains a massive 755,000 plus observations for loan applications that were rejected by Lending Club between 2007 and 2012. Although there is a significant number of observations there is less features available than with the Approved Data. The followinga are the Rejected Data features (which are all also used in the Approved Data):
  
      * Amount requested
      * Application date
      * Loan title
      * Risk score - FICO is used as the measurement for loan requests up until Nov-13. Since the data chosen is up to 2013,           FICO is the measure used and hence is comparable with the Approved Data set. 
      * DTI ratio
      * Zip code
      * State
      * Employment length
      * Policy code
      
## Data Limitations

The main limitations with the 2 data sets is the lack of common features. The Rejected Data set only has 9 features which can be compared to the Approved Data set. Features such as Previous Delinquency History, Days Delinquent and other variables that give a better insight into the applicants credit history would be useful for the Rejected Data set. Although Loan Title is included in both data sets, the lack of the Purpose feature will also be a limitation as this column has far fewer variables and hence can be easily grouped into a manageable number of observations for the comparative analysis. 

From the initial exploration of the available data, the following features will need to be analysed in greater detail and will be useful for the comparative analysis:

      * FICO
      * DTI
      * Amount
      * Employment length


## Data Wrangling and Cleaning 

The data sets were relatively comprehensive and well put together and hence did not require any major transformations. 

However, there were quite a few features that would not be required for the analysis, along with features that were made up primarily of NA values. Formatting was required on several columns as well as the application of One Hot Encoding which would aid the predictive model testing later in the project. 

The main libraries required for the data wrangling and cleaning were acquired when the relevant data set was imported. 

```{r eval =FALSE}
library(tidyr)
library(dplyr)
library(readr)
LC_data <- read_csv("~/Springboard/Capstone project/LC data.csv")

```
The following shows the steps undertaken to transform the data into a desirable format suitable for the analysis.

### Deleting features
The first task was to delete any obvious columns that would not be of any benefit for the analysis. Once this was done, further investigation into each feature was carried out to determine the percentage of "NA" fields, deleting anything greater than 80% NA.  

```{r eval = FALSE}
lc = LC_data
missing_values <- sapply(lc, function(x) {
  percentage <- sum(is.na(x))/length(x)
  percentage < 0.2
  })
lc <- lc[, missing_values == TRUE]
lc
```
Any features with only one unique value were also deleted from the data set, as these would not add any value to the analysis.

```{r eval = FALSE}
unique_values2 <- sapply(lc, function(x) {
size <- length(unique(x[!is.na(x)]))
size > 1
   })
lc <- lc[, unique_values2 == TRUE]
```
### Formatting
The main formatting carried out on the data sets involved transforming variables from a character to a numberic format. This first involved removing any non numeric text/values such as "yrs", "months" and "%", "+" and ">" signs. The main variables that were formatted in this manner were interest rate, term, employment length and revolving utilisation.

```{r eval = FALSE}
lc$term <- gsub("months", "", lc$term)
lc$term <- as.numeric(lc$term)
```
The issue date feature "issue_d" was also formatted from a "character" to a "date" to make it compatible with the date column from the rejected data. 

One of the most valuable features from this data set is "loan status". This categorises each loan into one of seven observations to represent the current status of the loan. For the purpose of the analysis, these status' were separated into 2 groups - good indicators that represent if a loan is still on track to be fully paid off without default or delay, and bad indicators which represent default or delay. These 2 indicator groups were then assigned a binary in the new column "bad_indicators". This will be very helpful in the analysis, particularly for the development of the predictive model.

```{r eval = FALSE}
bad_indicators <- c("Late (16-30 days)", "Late (31-120 days)", "Default", "Charged Off")
lc$is_bad <- ifelse(lc$loan_status %in% bad_indicators, 1,
                    ifelse(lc$loan_status=="", NA,
                   0))
```
### Data normalisation and One Hot Encoding
In order to improve prediction accuracy in the predictive model, the FICO feature was normalised. The standardizing method was used as it allows for centre and scale to be specified in the code and hence, is easily transferable between different data sets (i.e. between approved and rejected data). 

```{r eval = FALSE}
lc$fico_norm <- scale(lc$fico_range_mean, center = TRUE, scale = TRUE)

```
The final step in the data wrangling was to apply One Hot Encoding to features of paricular interest. The rule of thumb is that OHE can be applied to features of between 2 and 30 unique values. 

```{r eval =FALSE}

library(ade4)

lc_OHE = lc
  ohe_feats = c('term', 'grade', 'emp_length_yrs', 'home_ownership', 'verification_status',   'loan_status', 'pymnt_plan', 'purpose', 'delinq_2yrs', 'inq_last_6mths', 'pub_rec', 'pub_rec_bankruptcies')
for(f in ohe_feats) {
     df_dummy1 = acm.disjonctif(as.data.frame(as.factor(unlist(lc[f])))) 
     lc_OHE = cbind(lc_OHE, df_dummy1)

}
```

The addition of OHE is to facilitate the analysis and testing in the predictive modelling section of the project. For the purpose of the initial exploratory analysis and data visualisation, the original columns will be used. 


Similiar wrangling was carried out for the rejected loan data set. 

### Approved and Rejected Data - Combined Data Set
The 2 data sets were then combined using the union() function. Firstly, a "status" feature was added to both data sets, which was set to either "rejected" or "approved" depending on the data set. Just the relevant columns were selected from the approved data set, and the names of both columns were changed to allow for the columns to be joined together. 

```{r eval = FALSE}
lc_comparison = lc
lc_comparison <- select(lc_comparison, emp_length_yrs, loan_amnt, issue_d, fico_range_mean, loan_status, dti)
lc_comparison <- lc_comparison  %>% mutate("Status" = "approved")
lc_comp <- union(lcr, lc_comparison)
```
To complete the combined data set wrangling, the FICO score was normalised as before with the approved data set. There was no need to specify the centre and scale as this encompasses all observations for both accepted and rejected loans. 


# Data Exploration
The purpose of the inital exploration was to gauge the relationship between the loan status and the remaining variables in the approved data set to determine if there are any factors that have more of an influence on the status of a loan than others. The objective of this was to take a selection of the more influencive variables in the data set to crudely determine the variables that could have the biggest impact on the success on the loan. 

Once determined, the focus of the further data visualisation and exploration will be to deep dive into these variables and determine how they interact with one another. 

The following are the variables to be investigated in this section:
      * fico_mean                       * grade                       
      * dti                             * home ownership
      * loan amnt                       * ann_income
      * emp length                      * inq. last 6 mths
      * term                            * revol_util
      * int rate                        * purpose

This will be done by plotting the density of 2 factors in is_bad (0/good/red and 1/bad/blue)) against each of the variables of interest. 
      
### Fico_mean
  
```{r eval = TRUE, echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
#install.packages("readr")
#library(readr)
lc_cleaned <- read.csv("~/Springboard/Capstone project/New folder/lc_cleaned.csv")

lc = lc_cleaned


library(ggplot2)

p <- ggplot(lc, aes(x = fico_range_mean, group = is_bad, color = factor(is_bad))) + geom_density()
p
```

As expected there is a difference between the 2 status' for this variable which suggests that a higher FICO score is more desirable with higher rates of defaulted loans witnessed in lower FICO ranges.

### DTI
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}

p1 <- ggplot(lc, aes(x = dti, group = is_bad, color = factor(is_bad))) + geom_density()
p1  
```

DTI is quite evenly spread however there is a slight tendency of higher DTI's to result in more bad loans hence confirming that the lower the DTI value the more likely the loan is to be successfully paid off. It is worth looking into further. 

### Loan Amount
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p2 <- ggplot(lc, aes(x = loan_amnt, group = is_bad, color = factor(is_bad))) + geom_density()
p2
```

Apart from loans of smaller amounts having more of a success of being fully paid off, the spread is quite even along the graph and does not suggest much of a relationship between the chance of failure and success.

### Employment length
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p3 <- ggplot(lc, aes(x = emp_length_yrs, group = is_bad, color = factor(is_bad))) + geom_density()
p3
```

Similar to Loan Amount, apart from 10 plus years of employment, there is no stark difference between the 2 observations. 

### Term
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p4 <- ggplot(lc, aes(x = term_mths, group = is_bad, color = factor(is_bad))) + geom_density()
p4
```

Interestingly, the shorter the term, the more likely the loan is to succeed. This is definitely worth looking into further. 

### Interest Rate
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p5 <- ggplot(lc, aes(x = int_rate_percent, group = is_bad, color = factor(is_bad))) + geom_density()
p5
```

Similar to DTI, it would be expected that the lower the interest rate on the loan, the greater the chance of success as the interest rate is assigned based on the risk of the loan which is derived from the applicant's credit history. 

### Grade
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p6 <- ggplot(lc, aes(x = grade, group = is_bad, color = factor(is_bad))) + geom_density()
p6
```

Grade appears to be similar to the interest rate in the fact that the grades which are deemed to be less risky are less likely to default i.e. A-grades are less likely to default while G-grades are more likely to default. It may be worth looking at the relationship between grade and interest rate later in the data exploration to determine the correlation between the two. 

### Home Ownership
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p7 <- ggplot(lc, aes(x = home_ownership, group = is_bad, color = factor(is_bad))) + geom_density()
p7
```

The two main observations in "home_ownership" are "rent" and "mortgage". The expectation would be that home owners would be more likely to repay their loans (which is the case) while renters would be more likely to default (which is not the case). There is not as much of a difference between the 2 of these observations as expected.  

### Annual Income
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p8 <- ggplot(lc, aes(x = annual_inc, group = is_bad, color = factor(is_bad))) + geom_density()
p8
```

There appears to be a consistent correlation between incomes for good and bad loans hence, this will not be investigated further.  

### Inquiries in the last 6 months
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p9 <- ggplot(lc, aes(x = inq_last_6mths, group = is_bad, color = factor(is_bad))) + geom_density()
p9
```

This graph shows that if the applicant has 0 inquiries they have a better chance of a good loan. There is a relatively consistent trend after this where there is not a major difference between the number of inquiries and the success of the loan.

### Revolution util
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p10 <- ggplot(lc, aes(x = revol_util, group = is_bad, color = factor(is_bad))) + geom_density()
p10
```

There is a very clear difference here between good and bad loans. The graph suggests that the lower the Revol Util, the better chance of successfully repaying the loan. 

### Purpose
```{r echo = FALSE, warning= FALSE, error = FALSE, message = FALSE}
p11 <- ggplot(lc, aes(x = purpose, group = is_bad, color = factor(is_bad))) + geom_density()
p11
```

There are 4 main peaks in the graph with significant differences between the good and bad loan status. This will be looked at in more detail to determine what particular loan purposes result in a greater risk. 


Overall, the inital data exploration and visulatisation demonstrates that there are significant variables that contribute to a loan being good or bad. FICO, employment length, interest rate, loan term, grade and purpose all show a strong relationship with the loan status as they demostrate a significant difference between the good and bad indicators of the loan status. These variables need to be investigated further.   
_________________________________


# Data Analysis
For the purpose of this section, machine learning will be used to build a model which can then be used to demostrate the contributions of the variables described in the Data Exploration section even further.

I will be using the "lc_formatted" dataset as described in the Data Wrangling section. This dataset does not include the One Hot Encoding exercise that was carried out on the data. This may have been useful if a significant amount of variables were being used for the machine learning however for the purpose of the logistic regression, I will be selecting the variables that will be used. Logistic regression will format the variables as required. For example, "Purpose" will be automatically separated to show the signifiance of each individual loan purpose such as education, small business etc. 



```{r eval = TRUE, echo = FALSE, warning = FALSE}
library(tidyr)
library(dplyr)
library(readr)

lc_formatted <- read_csv("~/Springboard/Capstone project/New folder/lc_formatted.csv")

lc2 = lc_formatted
```

## Further formatting - Variables not required

For the date columns to be useful, they need to be binned into quarters or even months so that trends can be investigated in groups rather than on particular days. For this analysis, the date columns will not be investigated, however future work could investigate correlations between default rate and loans issued on certain dates for example. Hence, columns "issue date", "last_pymnt_d", "last_credit_pull_d" etc will be omitted from the regression in this instance. 

Address will also not be required for the purpose of this analysis so this will also be removed from the data set. 

```{r eval = TRUE, echo = FALSE, warning = FALSE}
lc2[, c("addr_state", "earliest_cr_line", "last_credit_pull_d", "last_pymnt_d", "X1")] <- NULL
```
## Logistic Regression

The initial logistic regression to be carried out is to gain a better insight into the default rates of loans. The probability of default will be calculated for each loan which will allow for further analysis in terms of determining the true return on investment of a loan (when the default is taken into account as well as the interest rate). This will allow for a more robust portfolio for investors and hence a better overall return on investment.  


The dependent variable which will be used for the inital logistic regression analysis is "is_bad", the variable which tells us whether or not the loan has defaulted. "1" means the loan has defaulted while "0" means the loan has been successfully paid off. The independent variables will be added around this dependent variable and re-iterated until a successful model has been created.

Once the model has been built, the "Test" dataset will be used for its evalution. 


### Step 1 - Get the Baseline 
Predict the most frequent outcome (i.e. default loan or complete loan) of all observations. This is done by counting the actual number of default loans vs the number of complete loans to give the accuracy of the dataset. 

```{r eval = TRUE, echo = TRUE, results = 'asis'}
cm_baseline = table(lc2$is_bad, sign(lc2$is_bad))
```
```{r eval = TRUE, echo = TRUE}
cm_baseline
```

This produces the following table. The use of the sign function can derive a smarter baseline however in this case, both tables (actual results and smart baseline) derive the same accuracy of 84.88%.

Looking at the table and results however, the baseline of 84.88% is quite high due to the fact that the majority of cases are 0. To make the prediction more insightful, the data will be undersampled in Traind_under. This will comprise 50% 0 values and 50% 1 values. This way the baseline accuracy will be 50%. This will give the prediction based on the model built alot more value.
  

## Step 2 - Split the data 
Using CaTools package, split the data into 2 sections - test data and train data. The ratio for the divide will be 80/20 i.e. 80% of the dataset will be made up of train data "Train" and the remaining 20% will be test data "Test".

```{r eval = FALSE, echo = FALSE, warning = FALSE, message = FALSE}
library(caTools)
library(ROSE)
```

```{r eval = TRUE, echo = TRUE, warning = FALSE, message = FALSE}
library(caTools)
library(ROSE)
set.seed(80)
split = sample.split(lc2$is_bad, SplitRatio = 0.80)
Train = subset(lc2, split == TRUE)
Train_under <- ovun.sample(is_bad~., data = lc2, method="under", N = 12000, seed = 40)$data
Test = subset(lc2, split == FALSE)
```

Train data is the data set based on the original lc2 data. Alongside this data set, Train_under will also be used as this will give a clearer indication of the accuracy of the model that is built for reasons discussed in step 1. Train_under will be used to refine the model which will be derived from the Train data set.

## Step 3 - Run logistic regression on the model

Model1 will take into account all variables apart from member id and loan status. The idea is to start with the majority of the variables and gradually narrow down until only the significant variables are included in the model.

### Model 1
```{r eval = TRUE, echo = TRUE, warning=FALSE}
 model1 <- glm(is_bad ~. -id -loan_status, data = Train, family = "binomial")
summary1 = summary(model1)
summary1
```

Model1 gave several significant values however it did not converge hence the next step is to take just the significant values and run the regression model again until all values in the model are significant.

### Model 2
```{r eval = TRUE, echo = TRUE}
 model2 <- glm(is_bad ~ loan_amnt + term_mths + int_rate_percent + installment + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train, family = "binomial")
summary2 = summary(model2)
summary2

```

### Model 3
```{r eval = TRUE, echo = TRUE}
model3 <- glm(is_bad ~ loan_amnt + term_mths + int_rate_percent + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train, family = "binomial")

summary3 = summary(model3)
summary3
```

### Model 4
```{r eval = TRUE, echo = TRUE}
model4 <- glm(is_bad ~ loan_amnt + term_mths + purpose + int_rate_percent + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train, family = "binomial")

summary4 = summary(model4)
summary4

```

### Model 5
Running this model on the Train_under data to see how it compares:
```{r eval = TRUE, echo = TRUE}
model5 <- glm(is_bad ~ loan_amnt + term_mths + purpose + int_rate_percent + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train_under, family = "binomial")

summary5 = summary(model5)
summary5

```
After several iterations, model4 is a good fit and delivers an accurate model as the AIC, residual deviance and NULL deviance have all reduced. However, a stronger model can be achieved using an altered data set ("Train_under") which has been undersampled. Comparing model 4 to model 5 (where the undersampled data is used), the accuracy has improved based on the AIC score reduction from 24716 in the Train data to 9927.4. Similarly, the goodness of fit of the model has improved as both the NULL deviance and residual deviance have reduced with the use of the Train_under data. 

Getting the exp of the coefficients to make the data more readable:
```{r eval = TRUE, echo = TRUE}
model5_tab <- coef(summary(model5))
model5_tab[, "Estimate"] <- exp(coef(model5))
model5_tab
```

The higher the value, the greater the impact on the model and the overall probability of a loan defaulting. Looking at the table above, it is evidence that most significant variables in terms of the reason for the loan are "purpose_education", "purpose_small_business" and "purpose_renewable_energy". "Term_60mths" is also very significant which shows that the longer term, the greater the probabilty that the loan will not be repaid. Hence, there is a higher probability of default occuring when these observations of the variables are observed on a loan application. 


## Step 4 - Predicting on Training Data

As model5 was strong, this will be chosen for the preiction stage on the Train data. The na's will be removed to ensure all columns are of equal length. The resulting model6 will be used to obtain a confusion matrix for analysis.

```{r eval = TRUE, echo = TRUE}
model6 <- glm(is_bad ~ loan_amnt + term_mths + purpose + int_rate_percent + grade + annual_inc + total_acc + last_fico_range_high + meet_cred_pol + fico_norm, data = Train_under, family = "binomial", na.action = na.exclude)
summary6 <- summary(model6)

```
Use the predict function on the Test data with a threshold value of 0.5 to begin with. This will be adjusted for optimal position.
```{r eval = TRUE, echo = TRUE}
predict1 = predict(model6, type = "response", newdata = Train)
cmTrain <- table(Train$is_bad, predict1 > 0.5)
```

```{r eval = TRUE, echo = FALSE}
cmTrain
```

```{r eval = TRUE, echo = TRUE}
predictTest1 = predict(model6, type = "response", newdata = Test)
cmTest <- table(Test$is_bad, predictTest1 > 0.5)
```

```{r eval = TRUE, echo = FALSE}
cmTest
```

This matrix allows for calculations to be made regarding the accuracy of the model.
          
## Step 5 - Measuring the accuracy of the model
From the table above, the accuracy of the model can be measured using the following metrics.

### Overall accuracy 
Overall accuracy = (TN + TP)/N

```{r eval = TRUE, echo = FALSE }
(23500+4231)/nrow(Train)
(5932+1063)/nrow(Test)
```
Train Accuracy = 81.49%.
Test Accuracy = 82.19%

### Sensitivity 
Sensitivity = TP/(TP+FN)

```{r eval = TRUE, echo = FALSE}
4231/(913+4231)
1063/(1063+222)
```
Train Sensitivity = 82.25%
Test Sensitivity = 82.72%

### Specificity
Specificity = TN/(TN+FP)

```{r eval = TRUE, echo = FALSE}
23500/(23500+5360)
5932/(5932+1285)

```
Train Specificity = 81.43%
Test Specificity = 82.19%

Comparing the accuracy of the model using the Train data set to the baseline model shows:
Baseline accuracy = 50%
Model = 81.49%

Hence, the calculations show that the model is more accurate than the baseline model which means that the model will derive less mistakes when predicting the outcome of the success of a loan.

Comparing the Test and Train data shows good consistency with both data sets and hence, gives more confidence in the stabililty of the model.  

Want to go a step further with the analysis and investigate the threshold value (t) and the impact it can have on the accuracy and effectiveness of the model.

## Step 6 - Threshold Value

### AUC
In order to investigate the t value further, a good place to start is to determine the area under the curve (AUC) of the ROC curve for the model. The resulting AUC value is an indicator of the discrimination ability of the model. The value ranges from 0.5 to 1 with a higher value representing a better discrimination ability and hence a better predictor. 


For example, an ROC curve with an AUC of 0.5 will have a curve close to 45 degrees and will have little or no discimination ability i.e. the prediction will be 50/50 - pure chance.
Whereas a curve that tends towards the top left hand corner of the plot will have an AUC of closer to 1 which indicates perfect discrimination ability.
Hence, a high AUC value is more desirable as it indicate a better predictor power for the model in question.

```{r eval = FALSE, echo = FALSE, warning = FALSE, error = FALSE}
#![AUC Explanation.](\C:\Users\Pat\Documents\Springboard\Capstone project\Capstone #Milestone\README_figs_lreg\AUC.png)
```

Using the Train data to get the AUC:

```{r eval = TRUE, echo = TRUE, warning = FALSE, error = FALSE, message = FALSE}
library(ROCR)
pred = ROCR::prediction(predict1, Train$is_bad)
plotAUC <- as.numeric(performance(pred, "auc")@y.values)
plotAUC
```
This gives a value of 89.14% - this is higher than our baseline but can still be improved.

For a bank, the cost of the misclassifiction of default loans is much higher than the cost of the misclassification of successful loans i.e. it is more detrimental to Lending Club's business and to investors if default loans are predicted as successful rather than successful loans being predicted as default. 

Hence, for this particular model, sensitivity is very important to ensure the number of False negative cases in the model is kept as low as possible. This can be improved by adjusting the t value:

Investigating overall accuracy, sensivity, and specificity for different t values:

```{r eval = TRUE, echo = TRUE}
predict1 = predict(model6, type = "response", newdata = Train)
cm_t1 <- table(Train$is_bad, predict1 > 0.3)
cm_t2 <- table(Train$is_bad, predict1 > 0.4)
cm_t3 <- table(Train$is_bad, predict1 > 0.5)
cm_t4 <- table(Train$is_bad, predict1 > 0.75)
```

#### t>0.3
```{r eval = TRUE, echo = FALSE}
cm_t1
```

#### t>0.4
```{r eval = TRUE, echo = FALSE}
cm_t2
```

#### t>0.5
```{r eval = TRUE, echo = FALSE}
cm_t3
```

#### t>0.75
```{r eval = TRUE, echo = FALSE}
cm_t4
```

Computing these values and compiling the data into a table so that accuracy, sensitivity and specificity can be compared:

```{r eval = TRUE, echo = FALSE}
tvalue <- matrix(c(71, 85, 81, 86, 94, 89, 82, 57.6, 67, 75, 81, 91), ncol = 3, byrow= FALSE)
colnames(tvalue) <- c("Accuracy", "Sensitivity", "Specificity")
rownames(tvalue) <- c("t=0.3", "t=0.4", "t=0.5", "t=0.75")
tvalue <- as.table(tvalue)
tvalue
```


The main priority in this case is keeping the number of FN cases as low as possible. FN means that the model predicted the loan was good while in reality, the loan was actaully bad. There will always be some percentage of loans where the loan is predicted good but is actually bad however, we can adjust the t value to ensure this value is kept as low as possible.

Using t=0.3 gives a relatively low FN rate in comparison to other higher t-values however, the overall percentage accuracy is also lower than the other values. t=0.4 gives a 14% increase in accuracy with a 5% decrease in sensitivity. Somewhere between 0.3 and 0.4 will be the most suitable value for this model. This can be confirmed by visually inspecting the ROC curve.


### ROC Curve
In the calculations above, the t value of 0.5 was used. In this section, the ROC curve will be graphed which will allow for the t value to be investigated further. When choosing the t value, there is a trade off between sensitivity and specificity calculations. Hence, what is more detrimental to the success of Lending Club from the model - the cost of failing to detect positives or the cost of raising false alarms. 

Graphing the curve:

```{r eval = TRUE, echo = TRUE, warning= FALSE, error = FALSE, message = FALSE}
ROCRpred = ROCR::prediction(predict1, Train$is_bad)
ROCRperf = ROCR::performance(ROCRpred, "tpr", "fpr")

p2 <- plot(ROCRperf, colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj = c(-0.2, 1.7))
p2

```
The best cutoff to use for this curve that will give the best trade-off between sensitivity and specificity is t=0.35, as this is the point at which the curve levels off, after which the rate of True Positive values does not increase proportionally with False Positive values. 


The model can now be evaluated using the Test data set. 



## Step 7 - Testing the Model 

Computing the out-of-sample metrics on the Test dataset:

```{r eval = TRUE, echo = TRUE}
predictTest2 = predict(model6, type = "response", newdata = Test)
tableTest <- table(Test$is_bad, predictTest2 > 0.35)
```

```{r eval = TRUE, echo = TRUE}
tableTest
```
  
Overall accuracy = 75.7%

Sensitivity = 92.5%

Specificity = 72.9%

At a cutoff of 0.35, the sensitivity is massively improved from 82.7% in the original model to 92.5% in the revised model. This does not come with a massive trade off in overall model accuracy which has only dropped by 6.3% from the original model to 75.7%.


Can complete this section by adding the predicted default probabilities to the Train data set:

```{r}
Train$predicted.risk <- predict(model6, type = "response", newdata = Train)
```

# Results


### Determine the expected rate of return on the investment

For lending club to better screen applicants in future, the predicted risk of default of the loans can be used to determine how likely each loan is to default. The probability of default can also be used to determine the expected rate of return of the loan. This is considerably more insightful than the interest rate as it takes into account the probability of default rather than relying on factors used to set the interest rate such as FICO score.



First, derive formula to compute the Expected Rate of Return (ERR):

This formula is 
    **ERR = (1-predicted.risk)*((int_rate_percent)/100) + (predicted.risk)*(-1))**
    
```{r eval = TRUE, echo = TRUE}

ERR1 <- function(x, y) {
    ((1-x)*((y)/100) + (x)*(-1))
}

Train$ERR <- ERR1(Train$predicted.risk, Train$int_rate_percent)
```

```{r  eval = FALSE, echo = FALSE}
#write.csv(Train, file = "log_reg_train.csv")
```

### Determine other metrics

Can use the ERR to compute the expected profit. The results will give both positive and negative values with positive indicating an overall profit and negative indicating an overall loss. The overall amount returned is also calculated along with the amount returned based on no default i.e. just going by the interest rate where every loan completes and is fully paid. 

    **Profit = ERR*loan_amnt**
    
    **Amount_Returned = (1+ERR)*(loan_amnt)**
    
    **Amount_Returned_no_default = (1 + (int_rate/100))*(loan_amnt)**

```{r eval = TRUE, echo = TRUE}

Train$profit <- Train$ERR*Train$loan_amnt

Train$amnt_rtrnd <- (1+Train$ERR)*Train$loan_amnt

Train$amnt_rtrnd_no_default <- (1 + (Train$int_rate_percent/100))*(Train$loan_amnt)

```

### Thresholding

These loans can then be investigated further using the ERR metric and default rate in order to set the most suitable threshold for accepting loans - the threshold value describes the cut off after which no further loans are accepted. 

```{r eval = TRUE, echo = FALSE, message=FALSE, warning = FALSE}
#install.packages("distr")
library(distr)

```
Carrying out the thresholding on the Train data set for the variables:


### Predicted Risk

```{r eval = TRUE, echo = TRUE, warning = FALSE}
Threshold1 <- quantile(Train$predicted.risk, probs = seq(0, 1, 0.01), na.rm = TRUE,
          names = TRUE, type = 7)
Threshold1


tplot1 <- plot(c(1:1000), quantile(Train$predicted.risk, na.rm = TRUE, probs=c(1:1000)/1000))
tplot1
```

As calculated in section 6, the optimal threshold value for the predicted risk is t=0.35. This value corresponds to an accuracy of 76.7% and a sensitivity of 92.5%. Other t-values resulted in a higher accuracy however for the purpose of this project, it is vital that the rate of FN loans (i.e. loans that are default but predicted as good) is kept as low as possible. 

Looking at the quantile distribution above, a t-value of 0.35 corresponds to 62% i.e. 62% of loans have a predicted risk below the cut off value of 0.35. 

### ERR

```{r eval = TRUE, echo = TRUE}
Threshold2 <- quantile(Train$ERR, probs = seq(0, 1, 0.01), na.rm = TRUE,
         names = TRUE, type = 7)
Threshold2


tplot2 <- plot(c(1:1000), quantile(Train$ERR, na.rm = TRUE, probs=c(1:1000)/1000))
tplot2
```

The ERR quantile distribution is slightly different. For example, for the worst 10% of loans, there is an expected loss of 85% form the orignal sum of the loan. The ERR is predicted as negative for the first 69% of loans which means that there will be an overall loss for the worst 69% of accepted loans. Hence, in order to maximise the profit, 100% of loans should be accepted. 


Can also see the quantile distribution for the interest rate, amount returned and amount returned assuming no default.

### Interest Rate

```{r eval = TRUE, echo = FALSE}
Threshold3 <- quantile(Train$int_rate_percent, probs = seq(0, 1, 0.1), na.rm = TRUE,
          names = TRUE, type = 7)
knitr :: kable(Threshold3) 


tplot3 <- plot(c(1:1000), quantile(Train$int_rate_percent, na.rm = TRUE, probs=c(1:1000)/1000))
tplot3
```


### Amount Returned 

```{r eval = TRUE, echo = FALSE}
Threshold4 <- quantile(Train$amnt_rtrnd, probs = seq(0, 1, 0.1), na.rm = TRUE,
          names = TRUE, type = 7)
knitr :: kable(Threshold4) 


tplot4 <- plot(c(1:1000), quantile(Train$amnt_rtrnd, na.rm = TRUE, probs=c(1:1000)/1000))
tplot4
```


### Amount Returned assuming no default

```{r eval = TRUE, echo = FALSE}
Threshold5 <- quantile(Train$amnt_rtrnd_no_default, probs = seq(0, 1, 0.1), na.rm = TRUE,
          names = TRUE, type = 7)
knitr :: kable(Threshold5) 


tplot5 <- plot(c(1:1000), quantile(Train$amnt_rtrnd_no_default, na.rm = TRUE, probs=c(1:1000)/1000))
tplot5
```


# Conclusion and Next Steps

The model which has been built for predicting the probability of default for each loan has been successfully applied to the Lending Club data. The model has outperformed the baseline by 25.7% in terms of accuracy with an optimal sensitivity of 92.5% achieved. 

As discussed in section 6 for the optimisation of the t-value, the sensitivity of the model was very important and needed to be as high as possible. The cost of misclassification of a default loan can be much more detrimental to Lending Club than the cost of misclassification of a successful loan. Hence, it was far more important for the model to keep False Negatives as low as possible rather than False Positives, while still keeping the accuracy high. Hence, there was a trade-off between the 3 main metrics: Accuracy, Sensitivity and Specificity.

Accuracy, Sensitivity and Specificity were all calculated for various t-values to determine the optimal t-value that offered high values for each with a particular focus on the Sensitivity. 

0.3 < t > 0.5 was determined to be the region of the optimal t-value. The ROC curve was then used to visually inspect the relationship between the True Positive and the False Positive rates which showed the curve leveling off from t = 0.35. Hence, t = 0.35 was chosen. 

Ultimately, the model above calls into question the acceptance criteria of the loans. 62% of the loans were good based on the cut off value of t = 0.35 ie. 62% of loans had predicted risk below 0.35. From this calculation is would appear that Lending Club's screening process may need to be refined in order to keep the predicted loan risk below the cutoff value. 

With that said, interestingly, the quantile distribution for the ERR (Expected Rate of Return) tells a different story. 62% of the accepted loans result in a rate of return of -3.9%. Whereas if 100% of the loans are accepted, the overall rate of return is 18.9%. 

Taking these two variables into account, it is clear there is a trade-off between the ERR and the Predicted Risk. Hence, it depends on the criteria of Lending Club: to make as much profit overall, or to ensure fewer defaults for the individual investors.



## Next steps

The following are the next steps I would take to further investigate the Lending Club data:

1. Model comparison 
      * the logistic regression model has worked well and has an improved accuracy than the baseline as discussed. However, it would be interesting to run a GBM model to see the comparison between the two. 
      
2. Interest rate
      * in terms of the accepted loan data which was used for the purpose of this analysis, the interest rate plays a significant role in both the loan attractiveness to potential investors (i.e. how much profit can be made on a successful loan) and to the probability of default. This is mainly because it is calculated from other variables that describe an applicants credit history such as FICO score. 
      * hence, it would be interesting to analyse the interest rate and build a model to determine the main variables that impact the calculation of interest rate. This could then be compared to model 6 used in this project, to see how the interest rate impacts the overall probability of default and to determine if the interest rate can be refined in any way. 
      
3. Rejected data
      * the rejected data was wrangled in the early stages of the project and combined with common factors in the accepted data. It would be interesting to compare the 2 data sets to build a model that describes the acceptance criteria of a loan. The expectation would be to refine the acceptance criteria and improve the overall probability of default of the accepted loans. 
      * this would be done by removing the worst loans from the approved loan data set and adding in better loans from the rejected loan data set that meet the refined acceptance criteria from the model).
